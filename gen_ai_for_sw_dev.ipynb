{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56b3ba5-27b0-4d90-95e1-b3b5486dae7b",
   "metadata": {},
   "source": [
    "Traditional programming - rules + data -> **answers**\n",
    "\n",
    "Machine Learning - **answers** + data -> rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9ccab-5d17-4600-95fb-e6aee678ff22",
   "metadata": {},
   "source": [
    "Key ideas under transformers:\n",
    "1. **Attention**: Allows the model to focus on specific words when predicting the next word\n",
    "\n",
    "   Attention considers the relationships between words in a sentence to understand the context and predict the upcoming word.\n",
    "\n",
    "   For example, if you had a sentence like my little white fluffy dog ran towards my guest, the attention mechanism would allow for the model to focus on the adjectives, little, white, and fluffy when predicting the meaning of the noun dog. Text is broken down into pieces called tokens, which are mostly individual words, but they're occasionally pieces of words. Each word is assigned a high dimensional vector, which is a mathematical representation of the words meaning. This vector is called an embedding. The attention mechanism helps to adjust these embeddings to account for the context of the surrounding words. For example, here, little, white, and fluffy will impact the values of the dog vector, changing it to more resemble them.\n",
    "\n",
    "2. **Encoder**: Imagine you're working on a piece of code that's part of a complex code base. Before you add or modify anything, you need to understand the entire context, what each part of the code does and how it interacts with other parts. That's pretty much what the encoder in a transformer model will do with text. When we're training a transformer base model, the encoder takes the entire input sequence at once. Unlike traditional supervised models, that process data step by step, the encoder looks at all parts of the data simultaneously. This is possible, thanks to the attention mechanism. Remember, they allow the encoder to focus on different parts of the input sequencing, determining which features are the most important. You could see this process as somewhat like a code review, where you might focus more on the critical sections that could impact the functionality of the entire application. After processing, the encoder converts the input data into a set of context vectors. These vectors are a distilled representation of the input text, encapsulating the learned insights and relationships between different elements of the data thanks to the attention mechanism.\n",
    "\n",
    "3. **Decoder**: The decoder reverses this process. In a way you can think about this in terms of a code review and the insights that you've gained from it. You're no longer just reviewing you're planning what to code next based on the understanding that you've just developed. Through the many pieces of intelligence that are gleaned through the attention mechanism, the model understands a lot of context about what you're currently working on with it, and as a result, can intelligently suggest new content to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514e64e-b9ef-4ee3-97a9-1d83eb5212f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
